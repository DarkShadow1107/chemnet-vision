"""
Etapa 3: Preprocesarea Setului de Date pentru ReÈ›ele Neuronale
Proiect: ChemNet Vision - Analiza Moleculelor
Disciplina: ReÈ›ele Neuronale - POLITEHNICA BucureÈ™ti

Acest script realizeazÄƒ:
1. Analiza Exploratorie a Datelor (EDA)
2. CurÄƒÈ›area È™i Preprocesarea Datelor
3. ÃmpÄƒrÈ›irea Ã®n seturi train/validation/test
"""

import pandas as pd
import numpy as np
import os
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from rdkit import Chem
from rdkit.Chem import Descriptors, AllChem
import warnings
warnings.filterwarnings('ignore')

# Configurare cÄƒi
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
DATA_DIR = os.path.join(BASE_DIR, 'data')
RAW_DIR = os.path.join(DATA_DIR, 'raw')
PROCESSED_DIR = os.path.join(DATA_DIR, 'processed')
TRAIN_DIR = os.path.join(DATA_DIR, 'train')
VAL_DIR = os.path.join(DATA_DIR, 'validation')
TEST_DIR = os.path.join(DATA_DIR, 'test')
IMAGES_DIR = os.path.join(DATA_DIR, '2d_images')
CONFIG_DIR = os.path.join(BASE_DIR, 'config')
DOCS_DIR = os.path.join(BASE_DIR, 'docs', 'datasets')


def load_raw_data():
    """ÃncarcÄƒ datele brute din fiÈ™ierul CSV."""
    csv_path = os.path.join(DATA_DIR, 'molecules.csv')
    
    # CiteÈ™te cu separator punct-virgulÄƒ, gestionÃ¢nd linii problematice
    df = pd.read_csv(csv_path, sep=';', quotechar='"', on_bad_lines='skip')
    
    print(f"âœ… Date Ã®ncÄƒrcate: {len(df)} molecule")
    print(f"ğŸ“Š NumÄƒr caracteristici: {len(df.columns)}")
    
    return df


def exploratory_data_analysis(df):
    """
    RealizeazÄƒ Analiza Exploratorie a Datelor (EDA).
    ReturneazÄƒ un dicÈ›ionar cu statistici È™i probleme identificate.
    """
    print("\n" + "="*60)
    print("ğŸ“Š ANALIZA EXPLORATORIE A DATELOR (EDA)")
    print("="*60)
    
    eda_report = {
        'n_samples': len(df),
        'n_features': len(df.columns),
        'features': list(df.columns),
        'dtypes': df.dtypes.astype(str).to_dict(),
        'missing_values': {},
        'statistics': {},
        'problems': []
    }
    
    # 1. Analiza valorilor lipsÄƒ
    print("\nğŸ“Œ 1. Analiza valorilor lipsÄƒ:")
    print("-" * 40)
    
    missing = df.isnull().sum() + (df == '').sum() + (df == 'None').sum()
    missing_pct = (missing / len(df)) * 100
    
    for col in df.columns:
        pct = missing_pct[col]
        if pct > 0:
            eda_report['missing_values'][col] = {
                'count': int(missing[col]),
                'percentage': round(pct, 2)
            }
            print(f"  {col}: {missing[col]} ({pct:.2f}%)")
            
            if pct > 30:
                eda_report['problems'].append(f"Feature '{col}' are {pct:.1f}% valori lipsÄƒ (>30%)")
    
    # 2. Identificarea coloanelor numerice
    numeric_cols = ['Molecular Weight', 'Targets', 'Bioactivities', 'AlogP', 
                   'Polar Surface Area', 'HBA', 'HBD', '#RO5 Violations',
                   '#Rotatable Bonds', 'QED Weighted', 'Aromatic Rings',
                   'Heavy Atoms', 'Np Likeness Score']
    
    print("\nğŸ“Œ 2. Statistici descriptive pentru caracteristici numerice:")
    print("-" * 40)
    
    for col in numeric_cols:
        if col in df.columns:
            # ConverteÈ™te la numeric
            series = pd.to_numeric(df[col], errors='coerce')
            valid = series.dropna()
            
            if len(valid) > 0:
                stats = {
                    'min': float(valid.min()),
                    'max': float(valid.max()),
                    'mean': float(valid.mean()),
                    'median': float(valid.median()),
                    'std': float(valid.std()),
                    'q1': float(valid.quantile(0.25)),
                    'q3': float(valid.quantile(0.75))
                }
                stats['iqr'] = stats['q3'] - stats['q1']
                eda_report['statistics'][col] = stats
                
                print(f"\n  {col}:")
                print(f"    Min: {stats['min']:.2f}, Max: {stats['max']:.2f}")
                print(f"    Mean: {stats['mean']:.2f} Â± {stats['std']:.2f}")
                print(f"    Median: {stats['median']:.2f}")
                
                # Detectarea outlierilor folosind IQR
                lower_bound = stats['q1'] - 1.5 * stats['iqr']
                upper_bound = stats['q3'] + 1.5 * stats['iqr']
                outliers = ((valid < lower_bound) | (valid > upper_bound)).sum()
                
                if outliers > 0:
                    outlier_pct = (outliers / len(valid)) * 100
                    print(f"    âš ï¸ Outlieri: {outliers} ({outlier_pct:.1f}%)")
                    if outlier_pct > 5:
                        eda_report['problems'].append(
                            f"Feature '{col}' are {outlier_pct:.1f}% outlieri"
                        )
    
    # 3. Analiza caracteristicilor categoriale
    print("\nğŸ“Œ 3. Caracteristici categoriale:")
    print("-" * 40)
    
    categorical_cols = ['Type', 'Max Phase', 'Structure Type', 'Inorganic Flag',
                       'Passes Ro3', 'Withdrawn Flag', 'Orphan']
    
    for col in categorical_cols:
        if col in df.columns:
            unique_vals = df[col].nunique()
            print(f"  {col}: {unique_vals} valori unice")
            
            # VerificÄƒ dezechilibrul claselor
            value_counts = df[col].value_counts(normalize=True)
            if len(value_counts) >= 2:
                max_pct = value_counts.iloc[0] * 100
                if max_pct > 90:
                    eda_report['problems'].append(
                        f"Feature '{col}' are dezechilibru de clasÄƒ ({max_pct:.1f}% pentru clasa majoritarÄƒ)"
                    )
    
    # 4. Verificarea SMILES
    print("\nğŸ“Œ 4. Validarea structurilor SMILES:")
    print("-" * 40)
    
    valid_smiles = 0
    invalid_smiles = 0
    
    for smiles in df['Smiles']:
        if pd.isna(smiles) or smiles == '' or smiles == 'None':
            invalid_smiles += 1
        else:
            mol = Chem.MolFromSmiles(str(smiles))
            if mol is not None:
                valid_smiles += 1
            else:
                invalid_smiles += 1
    
    print(f"  âœ… SMILES valide: {valid_smiles} ({valid_smiles/len(df)*100:.1f}%)")
    print(f"  âŒ SMILES invalide: {invalid_smiles} ({invalid_smiles/len(df)*100:.1f}%)")
    
    if invalid_smiles > 0:
        eda_report['problems'].append(
            f"{invalid_smiles} molecule au SMILES invalide sau lipsÄƒ"
        )
    
    eda_report['valid_smiles'] = valid_smiles
    eda_report['invalid_smiles'] = invalid_smiles
    
    # 5. Rezumat probleme identificate
    print("\nğŸ“Œ 5. Probleme identificate:")
    print("-" * 40)
    
    if len(eda_report['problems']) == 0:
        print("  âœ… Nu au fost identificate probleme majore")
    else:
        for i, problem in enumerate(eda_report['problems'], 1):
            print(f"  {i}. âš ï¸ {problem}")
    
    return eda_report


def preprocess_data(df, eda_report):
    """
    CurÄƒÈ›Äƒ È™i preproceseazÄƒ datele.
    
    Etape:
    1. Eliminare duplicate
    2. Tratarea valorilor lipsÄƒ
    3. Validare È™i filtrare SMILES
    4. Normalizare caracteristici numerice
    5. Encoding variabile categoriale
    6. Extragere descriptori moleculari
    """
    print("\n" + "="*60)
    print("ğŸ”§ PREPROCESAREA DATELOR")
    print("="*60)
    
    df_processed = df.copy()
    preprocessing_log = {
        'original_samples': len(df),
        'steps': []
    }
    
    # 1. Eliminarea duplicatelor
    print("\nğŸ“Œ 1. Eliminarea duplicatelor...")
    initial_count = len(df_processed)
    df_processed = df_processed.drop_duplicates(subset=['ChEMBL ID'])
    removed = initial_count - len(df_processed)
    print(f"  Eliminate {removed} duplicate")
    preprocessing_log['steps'].append({
        'step': 'remove_duplicates',
        'removed': removed
    })
    
    # 2. Filtrarea moleculelor cu SMILES valid
    print("\nğŸ“Œ 2. Validarea È™i filtrarea SMILES...")
    valid_mask = []
    for smiles in df_processed['Smiles']:
        if pd.isna(smiles) or smiles == '' or smiles == 'None':
            valid_mask.append(False)
        else:
            mol = Chem.MolFromSmiles(str(smiles))
            valid_mask.append(mol is not None)
    
    df_processed = df_processed[valid_mask]
    print(f"  PÄƒstrate {len(df_processed)} molecule cu SMILES valid")
    preprocessing_log['steps'].append({
        'step': 'filter_valid_smiles',
        'remaining': len(df_processed)
    })
    
    # 3. Tratarea valorilor lipsÄƒ pentru coloane numerice
    print("\nğŸ“Œ 3. Tratarea valorilor lipsÄƒ...")
    
    numeric_cols = ['Molecular Weight', 'Targets', 'Bioactivities', 'AlogP', 
                   'Polar Surface Area', 'HBA', 'HBD', '#RO5 Violations',
                   '#Rotatable Bonds', 'QED Weighted', 'Aromatic Rings',
                   'Heavy Atoms', 'Np Likeness Score']
    
    imputation_stats = {}
    
    for col in numeric_cols:
        if col in df_processed.columns:
            # ConverteÈ™te la numeric
            df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')
            
            # CalculeazÄƒ median din date valide
            median_val = df_processed[col].median()
            missing_before = df_processed[col].isna().sum()
            
            # Imputare cu median
            df_processed[col] = df_processed[col].fillna(median_val)
            
            if missing_before > 0:
                imputation_stats[col] = {
                    'method': 'median',
                    'value': float(median_val),
                    'imputed_count': int(missing_before)
                }
                print(f"  {col}: {missing_before} valori imputate cu median ({median_val:.2f})")
    
    preprocessing_log['steps'].append({
        'step': 'impute_missing',
        'imputation_stats': imputation_stats
    })
    
    # 4. Tratarea outlierilor (folosind IQR capping)
    print("\nğŸ“Œ 4. Tratarea outlierilor (IQR capping)...")
    
    outlier_treatment = {}
    
    for col in numeric_cols:
        if col in df_processed.columns:
            q1 = df_processed[col].quantile(0.25)
            q3 = df_processed[col].quantile(0.75)
            iqr = q3 - q1
            
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            
            outliers_before = ((df_processed[col] < lower_bound) | 
                              (df_processed[col] > upper_bound)).sum()
            
            if outliers_before > 0:
                df_processed[col] = df_processed[col].clip(lower_bound, upper_bound)
                outlier_treatment[col] = {
                    'lower_bound': float(lower_bound),
                    'upper_bound': float(upper_bound),
                    'capped_count': int(outliers_before)
                }
                print(f"  {col}: {outliers_before} outlieri trataÈ›i")
    
    preprocessing_log['steps'].append({
        'step': 'outlier_treatment',
        'treatment': outlier_treatment
    })
    
    # 5. Encoding variabile categoriale
    print("\nğŸ“Œ 5. Encoding variabile categoriale...")
    
    encoding_maps = {}
    
    # Type (tipul moleculei)
    if 'Type' in df_processed.columns:
        le = LabelEncoder()
        df_processed['Type_encoded'] = le.fit_transform(df_processed['Type'].astype(str))
        encoding_maps['Type'] = dict(zip(le.classes_, range(len(le.classes_))))
        print(f"  Type: {len(le.classes_)} clase encoded")
    
    # Passes Ro3 (Regula lui 3)
    if 'Passes Ro3' in df_processed.columns:
        df_processed['Passes_Ro3_encoded'] = df_processed['Passes Ro3'].map({'Y': 1, 'N': 0})
        df_processed['Passes_Ro3_encoded'] = df_processed['Passes_Ro3_encoded'].fillna(0).astype(int)
        encoding_maps['Passes Ro3'] = {'Y': 1, 'N': 0}
        print(f"  Passes Ro3: binary encoded")
    
    # Structure Type
    if 'Structure Type' in df_processed.columns:
        le = LabelEncoder()
        df_processed['Structure_Type_encoded'] = le.fit_transform(
            df_processed['Structure Type'].astype(str)
        )
        encoding_maps['Structure Type'] = dict(zip(le.classes_, range(len(le.classes_))))
        print(f"  Structure Type: {len(le.classes_)} clase encoded")
    
    preprocessing_log['steps'].append({
        'step': 'categorical_encoding',
        'encoding_maps': encoding_maps
    })
    
    # 6. Extragerea descriptorilor moleculari din SMILES
    print("\nğŸ“Œ 6. Extragerea descriptorilor moleculari...")
    
    molecular_features = []
    
    for smiles in df_processed['Smiles']:
        mol = Chem.MolFromSmiles(str(smiles))
        if mol is not None:
            features = {
                'MolWeight_RDKit': Descriptors.MolWt(mol),
                'LogP_RDKit': Descriptors.MolLogP(mol),
                'TPSA_RDKit': Descriptors.TPSA(mol),
                'NumHDonors_RDKit': Descriptors.NumHDonors(mol),
                'NumHAcceptors_RDKit': Descriptors.NumHAcceptors(mol),
                'NumRotatableBonds_RDKit': Descriptors.NumRotatableBonds(mol),
                'NumAromaticRings_RDKit': Descriptors.NumAromaticRings(mol),
                'FractionCSP3': Descriptors.FractionCSP3(mol),
                'NumHeteroatoms': Descriptors.NumHeteroatoms(mol),
                'RingCount': Descriptors.RingCount(mol)
            }
        else:
            features = {k: 0 for k in ['MolWeight_RDKit', 'LogP_RDKit', 'TPSA_RDKit',
                                       'NumHDonors_RDKit', 'NumHAcceptors_RDKit',
                                       'NumRotatableBonds_RDKit', 'NumAromaticRings_RDKit',
                                       'FractionCSP3', 'NumHeteroatoms', 'RingCount']}
        molecular_features.append(features)
    
    mol_df = pd.DataFrame(molecular_features)
    df_processed = pd.concat([df_processed.reset_index(drop=True), mol_df], axis=1)
    
    print(f"  AdÄƒugaÈ›i 10 descriptori moleculari RDKit")
    
    preprocessing_log['steps'].append({
        'step': 'molecular_descriptors',
        'features_added': list(mol_df.columns)
    })
    
    # 7. Normalizarea caracteristicilor pentru ML
    print("\nğŸ“Œ 7. Normalizarea caracteristicilor...")
    
    features_to_normalize = numeric_cols + list(mol_df.columns)
    features_to_normalize = [f for f in features_to_normalize if f in df_processed.columns]
    
    # CalculÄƒm È™i salvÄƒm parametrii de normalizare
    normalization_params = {}
    
    for col in features_to_normalize:
        min_val = df_processed[col].min()
        max_val = df_processed[col].max()
        mean_val = df_processed[col].mean()
        std_val = df_processed[col].std()
        
        normalization_params[col] = {
            'min': float(min_val),
            'max': float(max_val),
            'mean': float(mean_val),
            'std': float(std_val)
        }
        
        # Normalizare Min-Max
        if max_val > min_val:
            df_processed[f'{col}_normalized'] = (df_processed[col] - min_val) / (max_val - min_val)
        else:
            df_processed[f'{col}_normalized'] = 0
    
    print(f"  Normalizate {len(features_to_normalize)} caracteristici (Min-Max)")
    
    preprocessing_log['steps'].append({
        'step': 'normalization',
        'method': 'min-max',
        'params': normalization_params
    })
    
    # 8. Corelarea cu imaginile 2D
    print("\nğŸ“Œ 8. Corelarea cu imaginile 2D...")
    
    # Construim calea cÄƒtre imagine pentru fiecare moleculÄƒ
    def get_image_path(name):
        if pd.isna(name) or name == '' or name == 'None':
            return None
        # Numele imaginii = Name.upper() + .png
        image_name = f"{str(name).upper()}.png"
        image_path = os.path.join(IMAGES_DIR, image_name)
        if os.path.exists(image_path):
            return image_path
        return None
    
    df_processed['image_path'] = df_processed['Name'].apply(get_image_path)
    
    # Statistici despre imagini
    images_found = df_processed['image_path'].notna().sum()
    images_missing = df_processed['image_path'].isna().sum()
    
    print(f"  Imagini gÄƒsite: {images_found} ({images_found/len(df_processed)*100:.1f}%)")
    print(f"  Imagini lipsÄƒ: {images_missing} ({images_missing/len(df_processed)*100:.1f}%)")
    
    # Flag pentru a indica dacÄƒ molecula are imagine
    df_processed['has_image'] = df_processed['image_path'].notna().astype(int)
    
    preprocessing_log['steps'].append({
        'step': 'image_correlation',
        'images_found': int(images_found),
        'images_missing': int(images_missing),
        'coverage_pct': round(images_found/len(df_processed)*100, 2)
    })
    
    preprocessing_log['final_samples'] = len(df_processed)
    preprocessing_log['final_features'] = len(df_processed.columns)
    
    return df_processed, preprocessing_log, normalization_params


def split_dataset(df, test_size=0.15, val_size=0.15, random_state=42):
    """
    Ãmparte datele Ã®n seturi train/validation/test.
    Train: 70%, Validation: 15%, Test: 15%
    """
    print("\n" + "="*60)
    print("ğŸ“Š ÃMPÄ‚RÈšIREA SETULUI DE DATE")
    print("="*60)
    
    # Prima Ã®mpÄƒrÈ›ire: train+val vs test
    train_val_df, test_df = train_test_split(
        df, 
        test_size=test_size, 
        random_state=random_state
    )
    
    # A doua Ã®mpÄƒrÈ›ire: train vs val
    val_ratio = val_size / (1 - test_size)
    train_df, val_df = train_test_split(
        train_val_df, 
        test_size=val_ratio, 
        random_state=random_state
    )
    
    split_info = {
        'train_size': len(train_df),
        'validation_size': len(val_df),
        'test_size': len(test_df),
        'train_pct': len(train_df) / len(df) * 100,
        'validation_pct': len(val_df) / len(df) * 100,
        'test_pct': len(test_df) / len(df) * 100,
        'random_state': random_state
    }
    
    print(f"\nğŸ“Œ DistribuÈ›ia seturilor:")
    print(f"  Train:      {split_info['train_size']} ({split_info['train_pct']:.1f}%)")
    print(f"  Validation: {split_info['validation_size']} ({split_info['validation_pct']:.1f}%)")
    print(f"  Test:       {split_info['test_size']} ({split_info['test_pct']:.1f}%)")
    
    return train_df, val_df, test_df, split_info


def save_processed_data(df_processed, train_df, val_df, test_df, 
                         eda_report, preprocessing_log, split_info,
                         normalization_params):
    """SalveazÄƒ toate datele È™i configuraÈ›iile."""
    print("\n" + "="*60)
    print("ğŸ’¾ SALVAREA REZULTATELOR")
    print("="*60)
    
    # 1. Salvare date brute (copie)
    raw_path = os.path.join(RAW_DIR, 'molecules_raw.csv')
    original_df = pd.read_csv(os.path.join(DATA_DIR, 'molecules.csv'), sep=';', on_bad_lines='skip')
    original_df.to_csv(raw_path, index=False)
    print(f"  âœ… Date brute: {raw_path}")
    
    # 2. Salvare date preprocesate
    processed_path = os.path.join(PROCESSED_DIR, 'molecules_processed.csv')
    df_processed.to_csv(processed_path, index=False)
    print(f"  âœ… Date preprocesate: {processed_path}")
    
    # 3. Salvare seturi train/val/test
    train_path = os.path.join(TRAIN_DIR, 'train.csv')
    val_path = os.path.join(VAL_DIR, 'validation.csv')
    test_path = os.path.join(TEST_DIR, 'test.csv')
    
    train_df.to_csv(train_path, index=False)
    val_df.to_csv(val_path, index=False)
    test_df.to_csv(test_path, index=False)
    
    print(f"  âœ… Set train: {train_path}")
    print(f"  âœ… Set validation: {val_path}")
    print(f"  âœ… Set test: {test_path}")
    
    # 4. Salvare raport EDA
    eda_path = os.path.join(DOCS_DIR, 'eda_report.json')
    with open(eda_path, 'w', encoding='utf-8') as f:
        json.dump(eda_report, f, indent=2, ensure_ascii=False)
    print(f"  âœ… Raport EDA: {eda_path}")
    
    # 5. Salvare log preprocesare
    log_path = os.path.join(DOCS_DIR, 'preprocessing_log.json')
    with open(log_path, 'w', encoding='utf-8') as f:
        json.dump(preprocessing_log, f, indent=2, ensure_ascii=False)
    print(f"  âœ… Log preprocesare: {log_path}")
    
    # 6. Salvare configuraÈ›ie preprocesare
    config = {
        'normalization_params': normalization_params,
        'split_info': split_info,
        'feature_columns': {
            'numeric': ['Molecular Weight', 'Targets', 'Bioactivities', 'AlogP', 
                       'Polar Surface Area', 'HBA', 'HBD', '#RO5 Violations',
                       '#Rotatable Bonds', 'QED Weighted', 'Aromatic Rings',
                       'Heavy Atoms', 'Np Likeness Score'],
            'categorical': ['Type', 'Structure Type', 'Passes Ro3'],
            'molecular_descriptors': ['MolWeight_RDKit', 'LogP_RDKit', 'TPSA_RDKit',
                                      'NumHDonors_RDKit', 'NumHAcceptors_RDKit',
                                      'NumRotatableBonds_RDKit', 'NumAromaticRings_RDKit',
                                      'FractionCSP3', 'NumHeteroatoms', 'RingCount'],
            'identifier': 'ChEMBL ID',
            'smiles': 'Smiles',
            'image_path': 'image_path',
            'has_image': 'has_image'
        },
        'images_dir': IMAGES_DIR
    }
    
    config_path = os.path.join(CONFIG_DIR, 'preprocessing_config.json')
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    print(f"  âœ… ConfiguraÈ›ie: {config_path}")
    
    # 7. Salvare caracteristici numerice pentru ML (format NumPy)
    normalized_cols = [col for col in df_processed.columns if col.endswith('_normalized')]
    
    X_train = train_df[normalized_cols].values
    X_val = val_df[normalized_cols].values
    X_test = test_df[normalized_cols].values
    
    np.save(os.path.join(TRAIN_DIR, 'X_train.npy'), X_train)
    np.save(os.path.join(VAL_DIR, 'X_val.npy'), X_val)
    np.save(os.path.join(TEST_DIR, 'X_test.npy'), X_test)
    
    print(f"  âœ… Caracteristici ML salvate Ã®n format NumPy")
    print(f"     X_train shape: {X_train.shape}")
    print(f"     X_val shape: {X_val.shape}")
    print(f"     X_test shape: {X_test.shape}")
    
    # 8. Salvare liste imagini pentru fiecare set (pentru training cu imagini)
    def save_image_list(df, set_name, output_dir):
        """SalveazÄƒ lista cÄƒilor cÄƒtre imagini pentru un set."""
        images_with_path = df[df['image_path'].notna()]['image_path'].tolist()
        
        # Salvare ca JSON
        list_path = os.path.join(output_dir, f'{set_name}_images.json')
        with open(list_path, 'w', encoding='utf-8') as f:
            json.dump({
                'count': len(images_with_path),
                'images': images_with_path
            }, f, indent=2, ensure_ascii=False)
        
        return len(images_with_path)
    
    train_images = save_image_list(train_df, 'train', TRAIN_DIR)
    val_images = save_image_list(val_df, 'validation', VAL_DIR)
    test_images = save_image_list(test_df, 'test', TEST_DIR)
    
    print(f"\n  âœ… Liste imagini salvate:")
    print(f"     Train: {train_images} imagini")
    print(f"     Validation: {val_images} imagini")
    print(f"     Test: {test_images} imagini")


def create_data_readme(eda_report, preprocessing_log, split_info):
    """GenereazÄƒ documentaÈ›ia dataset-ului."""
    
    readme_content = f"""# ğŸ“Š DocumentaÈ›ia Setului de Date - ChemNet Vision

## Descrierea Setului de Date

### Sursa datelor
* **Origine:** ChEMBL Database - Date despre molecule È™i compuÈ™i chimici
* **Modul de achiziÈ›ie:** FiÈ™ier extern (CSV)
* **Format original:** CSV cu separator punct-virgulÄƒ

### Caracteristicile dataset-ului original
* **NumÄƒr total de observaÈ›ii:** {eda_report['n_samples']}
* **NumÄƒr de caracteristici:** {eda_report['n_features']}
* **Tipuri de date:** Numerice, Categoriale, Text (SMILES)
* **Format fiÈ™iere:** CSV

---

## Caracteristici Principale

| CaracteristicÄƒ | Tip | Descriere | Domeniu valori |
|----------------|-----|-----------|----------------|
| ChEMBL ID | text | Identificator unic moleculÄƒ | - |
| Name | text | Numele moleculei | - |
| Molecular Weight | numeric | Masa molecularÄƒ (Da) | 0-2500 |
| AlogP | numeric | Coeficient de partiÈ›ie | -10 - 10 |
| Polar Surface Area | numeric | SuprafaÈ›a polarÄƒ (Ã…Â²) | 0-500 |
| HBA | numeric | Acceptori de hidrogen | 0-30 |
| HBD | numeric | Donori de hidrogen | 0-15 |
| #RO5 Violations | numeric | ÃncÄƒlcÄƒri regula lui 5 | 0-5 |
| Aromatic Rings | numeric | Inele aromatice | 0-10 |
| Smiles | text | Reprezentare SMILES | - |

---

## Analiza CalitÄƒÈ›ii Datelor

### Valori lipsÄƒ identificate
"""
    
    if eda_report['missing_values']:
        for col, info in eda_report['missing_values'].items():
            readme_content += f"* **{col}:** {info['count']} ({info['percentage']:.1f}%)\n"
    else:
        readme_content += "* Nu au fost identificate valori lipsÄƒ semnificative\n"
    
    readme_content += f"""
### Probleme identificate
"""
    
    if eda_report['problems']:
        for problem in eda_report['problems']:
            readme_content += f"* âš ï¸ {problem}\n"
    else:
        readme_content += "* âœ… Nu au fost identificate probleme majore\n"
    
    readme_content += f"""
---

## Preprocesarea Datelor

### Etape aplicate:
1. **Eliminarea duplicatelor** - Pe baza ChEMBL ID
2. **Validarea SMILES** - Filtrarea moleculelor cu structuri invalide
3. **Imputarea valorilor lipsÄƒ** - Metoda: medianÄƒ
4. **Tratarea outlierilor** - IQR capping (1.5 Ã— IQR)
5. **Encoding categorial** - LabelEncoder pentru variabile categoriale
6. **Extragerea descriptorilor moleculari** - 10 descriptori RDKit
7. **Normalizare** - Min-Max scaling

### Rezultat preprocesare:
* **ObservaÈ›ii iniÈ›iale:** {preprocessing_log['original_samples']}
* **ObservaÈ›ii finale:** {preprocessing_log['final_samples']}
* **Caracteristici finale:** {preprocessing_log['final_features']}

---

## ÃmpÄƒrÈ›irea Seturilor de Date

| Set | NumÄƒr probe | Procent |
|-----|-------------|---------|
| Train | {split_info['train_size']} | {split_info['train_pct']:.1f}% |
| Validation | {split_info['validation_size']} | {split_info['validation_pct']:.1f}% |
| Test | {split_info['test_size']} | {split_info['test_pct']:.1f}% |

**Random state:** {split_info['random_state']}

---

## Structura FiÈ™ierelor

```
data/
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ molecules_raw.csv          # Date originale
â”œâ”€â”€ processed/
â”‚   â””â”€â”€ molecules_processed.csv    # Date preprocesate
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ train.csv                  # Set de instruire
â”‚   â””â”€â”€ X_train.npy                # Caracteristici normalizate
â”œâ”€â”€ validation/
â”‚   â”œâ”€â”€ validation.csv             # Set de validare
â”‚   â””â”€â”€ X_val.npy                  # Caracteristici normalizate
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ test.csv                   # Set de testare
â”‚   â””â”€â”€ X_test.npy                 # Caracteristici normalizate
â””â”€â”€ README.md                      # AceastÄƒ documentaÈ›ie
```

---

## Descriptori Moleculari ExtraÈ™i

| Descriptor | Descriere |
|------------|-----------|
| MolWeight_RDKit | MasÄƒ molecularÄƒ calculatÄƒ cu RDKit |
| LogP_RDKit | Coeficient de partiÈ›ie calculat |
| TPSA_RDKit | SuprafaÈ›a polarÄƒ topologicÄƒ |
| NumHDonors_RDKit | NumÄƒr donori de hidrogen |
| NumHAcceptors_RDKit | NumÄƒr acceptori de hidrogen |
| NumRotatableBonds_RDKit | NumÄƒr legÄƒturi rotabile |
| NumAromaticRings_RDKit | NumÄƒr inele aromatice |
| FractionCSP3 | FracÈ›iunea carbonilor sp3 |
| NumHeteroatoms | NumÄƒr heteroatomi |
| RingCount | NumÄƒr total de inele |

---

*Generat automat de scriptul de preprocesare - Etapa 3*
*Data: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    # Salvare README
    readme_path = os.path.join(DATA_DIR, 'README.md')
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print(f"\n  âœ… DocumentaÈ›ie dataset: {readme_path}")
    
    return readme_content


def main():
    """FuncÈ›ia principalÄƒ care orchestreazÄƒ procesul de preprocesare."""
    print("\n" + "="*60)
    print("ğŸ§ª ChemNet Vision - Preprocesarea Datelor pentru RN")
    print("   Etapa 3: Analiza È™i PregÄƒtirea Setului de Date")
    print("="*60)
    
    # 1. ÃncÄƒrcare date
    df = load_raw_data()
    
    # 2. Analiza Exploratorie
    eda_report = exploratory_data_analysis(df)
    
    # 3. Preprocesare
    df_processed, preprocessing_log, normalization_params = preprocess_data(df, eda_report)
    
    # 4. ÃmpÄƒrÈ›ire Ã®n seturi
    train_df, val_df, test_df, split_info = split_dataset(df_processed)
    
    # 5. Salvare rezultate
    save_processed_data(
        df_processed, train_df, val_df, test_df,
        eda_report, preprocessing_log, split_info,
        normalization_params
    )
    
    # 6. Creare documentaÈ›ie
    create_data_readme(eda_report, preprocessing_log, split_info)
    
    print("\n" + "="*60)
    print("âœ… PREPROCESAREA COMPLETÄ‚!")
    print("="*60)
    print("\nğŸ“ FiÈ™iere generate:")
    print("   - data/raw/molecules_raw.csv")
    print("   - data/processed/molecules_processed.csv")
    print("   - data/train/train.csv, X_train.npy, train_images.json")
    print("   - data/validation/validation.csv, X_val.npy, validation_images.json")
    print("   - data/test/test.csv, X_test.npy, test_images.json")
    print("   - docs/datasets/eda_report.json")
    print("   - docs/datasets/preprocessing_log.json")
    print("   - config/preprocessing_config.json")
    print("   - data/README.md")
    print("\nğŸ–¼ï¸ Imaginile 2D sunt corelate È™i pot fi folosite pentru training!")
    print("\n")


if __name__ == "__main__":
    main()
